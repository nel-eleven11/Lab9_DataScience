{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0376e5c",
   "metadata": {},
   "source": [
    "# Laboratorio 9 Data Science \n",
    "\n",
    "## Nelson  García Bravatti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3951f86",
   "metadata": {},
   "source": [
    "# Fundamentos de Spark y Python — Laboratorio con DataFrames (Walmart Stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1a7a1",
   "metadata": {},
   "source": [
    "**Curso:** Data Science Sección 20\n",
    "\n",
    "**Duración estimada:** 1.5–2.5 horas\n",
    "\n",
    "**Modalidad:** Individual (colaboración para dudas conceptuales permitida, entrega individual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55db2a",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizaje\n",
    "Al finalizar, podrá:\n",
    "1. Iniciar una **SparkSession** y trabajar con **PySpark DataFrames**.\n",
    "2. **Cargar** un CSV con encabezados e inferencia de tipos.\n",
    "3. Explorar estructura: **columnas**, **esquema** y **muestras**.\n",
    "4. Ejecutar **descriptivos** y agregaciones.\n",
    "5. Aplicar **filtros**, **transformaciones** y **creación de columnas**.\n",
    "6. Calcular medidas estadísticas (p. ej., **correlación de Pearson**).\n",
    "7. Realizar consultas **temporales** (día con máximo precio; máximos por año).\n",
    "8. Comunicar hallazgos de forma ordenada y reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074eb1e",
   "metadata": {},
   "source": [
    "## Datos\n",
    "- **Archivo:** `walmart_stock.csv`\n",
    "- **Periodo:** 2012–2017\n",
    "- **Columnas típicas:** `Date`, `Open`, `High`, `Low`, `Close`, `Volume`, `Adj Close`\n",
    "\n",
    "> *Nota:* No modifique el CSV; todas las transformaciones se realizan en el notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc0d3f",
   "metadata": {},
   "source": [
    "## Requisitos previos\n",
    "- Python 3.9+ (o entorno equivalente en **Google Colab**)\n",
    "- **Apache Spark 3.x** con PySpark (o `pyspark` preinstalado en Colab)\n",
    "- Conocimientos básicos de: tipos de datos, funciones de agregación, y uso de notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc20a97",
   "metadata": {},
   "source": [
    "## Entregables\n",
    "1. **Notebook ejecutado** (`.ipynb`) con todas las celdas y salidas visibles.\n",
    "2. **Conclusiones breves** (5–10 líneas) al final del notebook con interpretaciones clave.\n",
    "3. Código **comentado** y ordenado.\n",
    "\n",
    "**Formato de entrega:** Subir `.ipynb` .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cb891",
   "metadata": {},
   "source": [
    "## 1) Configuración del entorno\n",
    "**Opción A — Local:**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab Spark DF — Walmart\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "```\n",
    "\n",
    "**Opción B — Colab (sugerida si no tiene Spark local):**\n",
    "```python\n",
    "!pip -q install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lab Spark DF — Walmart\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab Spark DF — Walmart\") \\\n",
    "    .config(\"spark.hadoop.security.authentication\", \"none\") \\\n",
    "    .config(\"spark.hadoop.security.authorization\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06abe683-36d4-4a60-a6b9-c0ac4c9218f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "WARNING: package sun.security.action not in java.base\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/08 19:05:51 WARN Utils: Your hostname, nelson-dell, resolves to a loopback address: 127.0.1.1; using 192.168.1.29 instead (on interface wlan0)\n",
      "25/10/08 19:05:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "WARNING: A restricted method in java.lang.System has been called\n",
      "WARNING: java.lang.System::loadLibrary has been called by org.apache.hadoop.util.NativeCodeLoader in an unnamed module (file:/home/nelson/Documents/Uvg/Data%20Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/jars/hadoop-client-api-3.4.1.jar)\n",
      "WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\n",
      "WARNING: Restricted methods will be blocked in a future release unless native access is enabled\n",
      "\n",
      "25/10/08 19:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLab Spark DF — Walmart\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark version:\u001b[39m\u001b[33m\"\u001b[39m, spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/core/context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/core/context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/pyspark/core/context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/py4j/java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Uvg/Data Science/Lab9_DataScience/venv/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1474)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Lab Spark DF — Walmart\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393768ea",
   "metadata": {},
   "source": [
    "## 2) Tareas (complete en orden y deje **toda** la evidencia en el notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4eb815",
   "metadata": {},
   "source": [
    "### 2.1 Inicie una sesión de Spark (si no está iniciada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92570384-193d-414b-abe1-b40b6845956c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd667b2",
   "metadata": {},
   "source": [
    "### 2.2 Cargue el archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57661cba-c17e-4027-8981-641611063cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV\n",
    "file_path = 'walmart_stock.csv'  \n",
    "df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d770e5e",
   "metadata": {},
   "source": [
    "### 2.3 ¿Cuáles son los nombres de las columnas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285e8b5-265c-4a4c-9607-dd13ae2726cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las columnas\n",
    "columns = df.columns\n",
    "print(\"Columnas:\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664b5e3",
   "metadata": {},
   "source": [
    "### 2.4 Muestre el **esquema** de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdf922-908e-4f24-b0ac-91527ec1a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el esquema de los datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1d127",
   "metadata": {},
   "source": [
    "### 2.5 Muestre las **primeras 5 filas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3c68d-2831-416d-9d07-c27b58d631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las primeras filas del dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca28ee8",
   "metadata": {},
   "source": [
    "### 2.6 Descriptivos con `describe()` + interpretación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9a12-2d3b-4fdf-856f-5de06cb7adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a4d9e2d",
   "metadata": {},
   "source": [
    "*(Escriba aquí sus interpretaciones de al menos dos métricas de `describe()`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527f08c-ab92-45ee-a80c-c16cf88887eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "311e3d2e",
   "metadata": {},
   "source": [
    "### 2.7 Máximo y mínimo de `Volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb1bdcf",
   "metadata": {},
   "source": [
    "### 2.8 ¿Cuántos días tuvieron `Close < 60`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4eafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a4df27",
   "metadata": {},
   "source": [
    "### 2.9 Crée la columna `Tasa_HV = High/Volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829902a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91d90513",
   "metadata": {},
   "source": [
    "### 2.10 ¿Qué porcentaje del tiempo `High > 80`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5e558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f57ee1dd",
   "metadata": {},
   "source": [
    "### 2.11 Correlación de Pearson entre `High` y `Volume` + interpretación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5c76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fad6d42b",
   "metadata": {},
   "source": [
    "*(Escriba aquí una interpretación breve del signo y magnitud de la correlación)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e6d94",
   "metadata": {},
   "source": [
    "### 2.12 ¿Qué día tuvo el **precio más alto** (`High`)? Devuelva la fila completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a8ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61337e44",
   "metadata": {},
   "source": [
    "### 2.13 **Media** de la columna `Close`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb18c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eaa741f",
   "metadata": {},
   "source": [
    "### 2.14 **Máximo `High` por año**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e4ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "846fa50f",
   "metadata": {},
   "source": [
    "## 3) Conclusiones (5–10 líneas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdad48c",
   "metadata": {},
   "source": [
    "*Escriba aquí sus principales hallazgos e interpretaciones.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4963c8",
   "metadata": {},
   "source": [
    "## Buenas prácticas\n",
    "- Comente bloques no triviales.\n",
    "- Nombres de variables **claros** (`df_prices`, `max_high_year`, etc.).\n",
    "- Reutilice resultados intermedios para evitar recalcular.\n",
    "- Si usa Colab, fije versiones cuando sea necesario.\n",
    "- Cierre la sesión de Spark al final si corre local: `spark.stop()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663a977",
   "metadata": {},
   "source": [
    "## Errores comunes\n",
    "- Olvidar `inferSchema=True` → todo se carga como `string`.\n",
    "- Mezclar API RDD con DataFrames sin necesidad.\n",
    "- Usar funciones de Python puras en `withColumn` (use `pyspark.sql.functions`).\n",
    "- Intentar graficar DataFrames de Spark directamente: primero **convierta** a Pandas con `.toPandas()` en subconjuntos pequeños.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d651d75",
   "metadata": {},
   "source": [
    "## Rúbrica de evaluación (100 puntos)\n",
    "**A. Preparación del ambiente** (10 pts)\n",
    "- (10) SparkSession creada sin errores; versiones y entorno claros.\n",
    "\n",
    "**B. Carga y documentación de datos** (15 pts)\n",
    "- (8) CSV cargado con `header` y `inferSchema` correctos.\n",
    "- (7) Comentarios breves sobre las columnas y supuestos.\n",
    "\n",
    "**C. Exploración básica** (10 pts)\n",
    "- (4) Lista de columnas.\n",
    "- (3) `printSchema()` bien interpretado.\n",
    "- (3) `show(5)` con observaciones puntuales.\n",
    "\n",
    "**D. Descriptivos** (10 pts)\n",
    "- (6) `describe()` ejecutado y leído correctamente.\n",
    "- (4) Al menos 2 interpretaciones numéricas.\n",
    "\n",
    "**E. Agregaciones y filtros** (10 pts)\n",
    "- (5) Máx./mín. de `Volume` correctos.\n",
    "- (5) Conteo de días con `Close < 60` correcto.\n",
    "\n",
    "**F. Ingeniería de características** (10 pts)\n",
    "- (8) Columna `Tasa_HV = High/Volume` correcta y con tipo numérico.\n",
    "- (2) Justificación breve del indicador.\n",
    "\n",
    "**G. Métricas estadísticas** (10 pts)\n",
    "- (7) Correlación `High`–`Volume` calculada.\n",
    "- (3) Interpretación del valor (signo y magnitud).\n",
    "\n",
    "**H. Consultas temporales** (10 pts)\n",
    "- (5) Día con `High` máximo identificado.\n",
    "- (5) Máximo `High` por año con agrupación y orden correctos.\n",
    "\n",
    "**I. Comunicación de resultados** (10 pts)\n",
    "- (6) Conclusiones finales claras y concisas (5–10 líneas).\n",
    "- (4) Orden, legibilidad y limpieza del notebook.\n",
    "\n",
    "**J. Estilo y calidad de código** (5 pts)\n",
    "- (5) Convenciones PEP8 razonables, nombres significativos y ausencia de código muerto.\n",
    "\n",
    "> **Total: 100 puntos**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c20de-edea-4fec-9e34-af294fd22482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
